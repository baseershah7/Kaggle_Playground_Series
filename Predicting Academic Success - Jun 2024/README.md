# Predicting Academic Success - June 2024

## Overview
This project is part of the Kaggle Playground Series (June 2024), focused on predicting academic success (Dropout, Graduate, Enrolled) using a rich tabular dataset of student and educational features. The notebook combines detailed EDA, feature engineering, advanced hyperparameter tuning, and stacking ensemble models for high-accuracy multiclass classification.

## Files
- `PSS4E6_stacking.ipynb`: Main notebook with all EDA, preprocessing, modeling, and submission steps.
- `submission.csv`: Kaggle submission file.

## Data Sources
- `/kaggle/input/playground-series-s4e6/train.csv`: Competition training data.
- `/kaggle/input/playground-series-s4e6/test.csv`: Competition test data.
- `/kaggle/input/playground-series-s4e6/sample_submission.csv`: Submission template.

## Dataset Description
**Features include:**
- Marital status
- Application mode/order
- Course
- Daytime/evening attendance
- Previous qualification (+ grade)
- Nacionality
- Parents' qualifications/occupations
- Admission grade
- Displaced, Educational special needs, Debtor, Tuition fees up to date
- Gender, Scholarship holder, Age at enrollment, International
- Curricular units (credited, enrolled, evaluations, approved, grade, without evaluations) for 1st/2nd semester
- Unemployment rate, Inflation rate, GDP
- **Target:** Academic success (Dropout, Graduate, Enrolled)

## Project Workflow

### 1. Exploratory Data Analysis (EDA)
- Statistical summaries and distribution analysis for all features.
- Identification of binary, categorical, long-categorical, and continuous features.
- Correlation analysis and (optional) feature importance, mutual information, and outlier detection.

### 2. Preprocessing & Feature Engineering
- Robust scaling for continuous features.
- Optional outlier removal via IsolationForest.
- Dropping irrelevant features (`Educational special needs`, `Nacionality`).
- Label encoding for target variable.
- Feature selection available via mutual information.

### 3. Model Validation & Hyperparameter Optimization
- Extensive cross-validation (KFold) for base models.
- Hyperparameter tuning for each major classifier using Optuna:
    - RandomForest, GradientBoosting, AdaBoost, ExtraTrees, LightGBM, CatBoost, XGBoost, HistGradientBoosting.
- Best hyperparameters recorded for stacking.

### 4. Stacking Ensemble Model
- Combines top-performing models (GradientBoosting, LightGBM, XGBoost, CatBoost, HistGradientBoosting) as base learners.
- Logistic Regression as meta-estimator.
- Trained on full dataset for robust multiclass classification.

### 5. Test Preprocessing & Prediction
- Test set processed with same pipeline (feature dropping, scaling).
- Predictions generated by stacking ensemble.
- Inverse label encoding to restore original class names.

### 6. Submission
- Kaggle submission file created with test IDs and predicted academic success labels.

## Results
- Target predictions: Dropout, Graduate, Enrolled (multiclass).
- Stacking ensemble with tuned hyperparameters achieves high accuracy.
- Example predictions:
    | id    | Target   |
    |-------|----------|
    | 76518 | Dropout  |
    | 76519 | Graduate |
    | 76520 | Graduate |

## How to Reproduce
1. Place all required datasets in the correct input directories.
2. Open `PSS4E6_stacking.ipynb` in Kaggle or Jupyter environment.
3. Run all cells in order to process data, engineer features, train models, and generate submission.

## Key Insights
- Stacking ensembles and Optuna-driven hyperparameter tuning boost model performance.
- Careful feature engineering and robust scaling improve generalization.
- Multiclass classification with high cardinality and continuous features requires thorough preprocessing.

## Future Work
- Explore additional feature engineering and selection.
- Incorporate more advanced meta-models for stacking.
- Analyze feature importances and interpretability.

## Author
- [baseershah7](https://github.com/baseershah7)
