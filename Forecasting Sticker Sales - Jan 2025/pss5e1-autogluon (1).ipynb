{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a223d11",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-25T11:16:09.724222Z",
     "iopub.status.busy": "2025-01-25T11:16:09.723954Z",
     "iopub.status.idle": "2025-01-25T11:16:10.406025Z",
     "shell.execute_reply": "2025-01-25T11:16:10.405004Z"
    },
    "papermill": {
     "duration": 0.688115,
     "end_time": "2025-01-25T11:16:10.407470",
     "exception": false,
     "start_time": "2025-01-25T11:16:09.719355",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/ensemble/weighted_ensemble.pkl\n",
      "/kaggle/input/test-pss5e1/test_pss5e1.csv\n",
      "/kaggle/input/playground-series-s5e1/sample_submission.csv\n",
      "/kaggle/input/playground-series-s5e1/train.csv\n",
      "/kaggle/input/playground-series-s5e1/test.csv\n",
      "/kaggle/input/train-pss5e1/train_pss5e1.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8db43a54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-25T11:16:10.415542Z",
     "iopub.status.busy": "2025-01-25T11:16:10.415210Z",
     "iopub.status.idle": "2025-01-25T11:16:22.004023Z",
     "shell.execute_reply": "2025-01-25T11:16:22.003144Z"
    },
    "papermill": {
     "duration": 11.594421,
     "end_time": "2025-01-25T11:16:22.005651",
     "exception": false,
     "start_time": "2025-01-25T11:16:10.411230",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting autogluon.tabular[0]\r\n",
      "  Downloading autogluon.tabular-1.2-py3-none-any.whl.metadata (14 kB)\r\n",
      "\u001b[33mWARNING: autogluon-tabular 1.2 does not provide the extra '0'\u001b[0m\u001b[33m\r\n",
      "\u001b[0mRequirement already satisfied: numpy<2.1.4,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from autogluon.tabular[0]) (1.26.4)\r\n",
      "Requirement already satisfied: scipy<1.16,>=1.5.4 in /usr/local/lib/python3.10/dist-packages (from autogluon.tabular[0]) (1.13.1)\r\n",
      "Requirement already satisfied: pandas<2.3.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from autogluon.tabular[0]) (2.2.2)\r\n",
      "Collecting scikit-learn<1.5.3,>=1.4.0 (from autogluon.tabular[0])\r\n",
      "  Downloading scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\r\n",
      "Requirement already satisfied: networkx<4,>=3.0 in /usr/local/lib/python3.10/dist-packages (from autogluon.tabular[0]) (3.4.2)\r\n",
      "Collecting autogluon.core==1.2 (from autogluon.tabular[0])\r\n",
      "  Downloading autogluon.core-1.2-py3-none-any.whl.metadata (12 kB)\r\n",
      "Collecting autogluon.features==1.2 (from autogluon.tabular[0])\r\n",
      "  Downloading autogluon.features-1.2-py3-none-any.whl.metadata (11 kB)\r\n",
      "Requirement already satisfied: tqdm<5,>=4.38 in /usr/local/lib/python3.10/dist-packages (from autogluon.core==1.2->autogluon.tabular[0]) (4.67.1)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autogluon.core==1.2->autogluon.tabular[0]) (2.32.3)\r\n",
      "Requirement already satisfied: matplotlib<3.11,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from autogluon.core==1.2->autogluon.tabular[0]) (3.7.5)\r\n",
      "Requirement already satisfied: boto3<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from autogluon.core==1.2->autogluon.tabular[0]) (1.35.93)\r\n",
      "Collecting autogluon.common==1.2 (from autogluon.core==1.2->autogluon.tabular[0])\r\n",
      "  Downloading autogluon.common-1.2-py3-none-any.whl.metadata (11 kB)\r\n",
      "Requirement already satisfied: psutil<7.0.0,>=5.7.3 in /usr/local/lib/python3.10/dist-packages (from autogluon.common==1.2->autogluon.core==1.2->autogluon.tabular[0]) (5.9.5)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<2.1.4,>=1.25.0->autogluon.tabular[0]) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<2.1.4,>=1.25.0->autogluon.tabular[0]) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<2.1.4,>=1.25.0->autogluon.tabular[0]) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<2.1.4,>=1.25.0->autogluon.tabular[0]) (2025.0.1)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<2.1.4,>=1.25.0->autogluon.tabular[0]) (2022.0.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<2.1.4,>=1.25.0->autogluon.tabular[0]) (2.4.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<2.3.0,>=2.0.0->autogluon.tabular[0]) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.3.0,>=2.0.0->autogluon.tabular[0]) (2024.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<2.3.0,>=2.0.0->autogluon.tabular[0]) (2024.2)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<1.5.3,>=1.4.0->autogluon.tabular[0]) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<1.5.3,>=1.4.0->autogluon.tabular[0]) (3.5.0)\r\n",
      "Requirement already satisfied: botocore<1.36.0,>=1.35.93 in /usr/local/lib/python3.10/dist-packages (from boto3<2,>=1.10->autogluon.core==1.2->autogluon.tabular[0]) (1.35.93)\r\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3<2,>=1.10->autogluon.core==1.2->autogluon.tabular[0]) (1.0.1)\r\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from boto3<2,>=1.10->autogluon.core==1.2->autogluon.tabular[0]) (0.10.4)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.2->autogluon.tabular[0]) (1.3.1)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.2->autogluon.tabular[0]) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.2->autogluon.tabular[0]) (4.55.3)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.2->autogluon.tabular[0]) (1.4.7)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.2->autogluon.tabular[0]) (24.2)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.2->autogluon.tabular[0]) (11.0.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.2->autogluon.tabular[0]) (3.2.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<2.3.0,>=2.0.0->autogluon.tabular[0]) (1.17.0)\r\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2.1.4,>=1.25.0->autogluon.tabular[0]) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2.1.4,>=1.25.0->autogluon.tabular[0]) (2022.0.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<2.1.4,>=1.25.0->autogluon.tabular[0]) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<2.1.4,>=1.25.0->autogluon.tabular[0]) (2024.2.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autogluon.core==1.2->autogluon.tabular[0]) (3.4.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autogluon.core==1.2->autogluon.tabular[0]) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autogluon.core==1.2->autogluon.tabular[0]) (2.2.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autogluon.core==1.2->autogluon.tabular[0]) (2024.12.14)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<2.1.4,>=1.25.0->autogluon.tabular[0]) (2024.2.0)\r\n",
      "Downloading autogluon.core-1.2-py3-none-any.whl (266 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.2/266.2 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading autogluon.features-1.2-py3-none-any.whl (64 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.1/64.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading autogluon.common-1.2-py3-none-any.whl (68 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.2/68.2 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m94.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading autogluon.tabular-1.2-py3-none-any.whl (352 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m352.2/352.2 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: scikit-learn, autogluon.common, autogluon.features, autogluon.core, autogluon.tabular\r\n",
      "  Attempting uninstall: scikit-learn\r\n",
      "    Found existing installation: scikit-learn 1.2.2\r\n",
      "    Uninstalling scikit-learn-1.2.2:\r\n",
      "      Successfully uninstalled scikit-learn-1.2.2\r\n",
      "Successfully installed autogluon.common-1.2 autogluon.core-1.2 autogluon.features-1.2 autogluon.tabular-1.2 scikit-learn-1.5.2\r\n"
     ]
    }
   ],
   "source": [
    "!pip install autogluon.tabular[0]\n",
    "# !pip install autogluon.timeseries[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cea7e61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-25T11:16:22.015587Z",
     "iopub.status.busy": "2025-01-25T11:16:22.015348Z",
     "iopub.status.idle": "2025-01-25T11:16:24.698410Z",
     "shell.execute_reply": "2025-01-25T11:16:24.697682Z"
    },
    "papermill": {
     "duration": 2.68957,
     "end_time": "2025-01-25T11:16:24.700045",
     "exception": false,
     "start_time": "2025-01-25T11:16:22.010475",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('/kaggle/input/train-pss5e1/train_pss5e1.csv')\n",
    "test = pd.read_csv('/kaggle/input/test-pss5e1/test_pss5e1.csv')\n",
    "train = train.dropna(subset='num_sold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f007910",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-25T11:16:24.709992Z",
     "iopub.status.busy": "2025-01-25T11:16:24.709765Z",
     "iopub.status.idle": "2025-01-25T11:16:24.842881Z",
     "shell.execute_reply": "2025-01-25T11:16:24.842206Z"
    },
    "papermill": {
     "duration": 0.139565,
     "end_time": "2025-01-25T11:16:24.844407",
     "exception": false,
     "start_time": "2025-01-25T11:16:24.704842",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train['time_id'] = train['country'] + '_' + train['product'] + '_' + train['store']\n",
    "test['time_id'] = test['country'] + '_' + test['product'] + '_' + test['store']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00ef0553",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-25T11:16:24.854178Z",
     "iopub.status.busy": "2025-01-25T11:16:24.853922Z",
     "iopub.status.idle": "2025-01-25T11:16:24.891886Z",
     "shell.execute_reply": "2025-01-25T11:16:24.891322Z"
    },
    "papermill": {
     "duration": 0.044211,
     "end_time": "2025-01-25T11:16:24.893202",
     "exception": false,
     "start_time": "2025-01-25T11:16:24.848991",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train['date'] = pd.to_datetime(train['date'])\n",
    "test['date'] = pd.to_datetime(test['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0eb48f05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-25T11:16:24.902468Z",
     "iopub.status.busy": "2025-01-25T11:16:24.902248Z",
     "iopub.status.idle": "2025-01-25T11:16:25.009449Z",
     "shell.execute_reply": "2025-01-25T11:16:25.008682Z"
    },
    "papermill": {
     "duration": 0.113285,
     "end_time": "2025-01-25T11:16:25.010853",
     "exception": false,
     "start_time": "2025-01-25T11:16:24.897568",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>country</th>\n",
       "      <th>store</th>\n",
       "      <th>product</th>\n",
       "      <th>num_sold</th>\n",
       "      <th>year</th>\n",
       "      <th>day_of_month</th>\n",
       "      <th>days_in_month</th>\n",
       "      <th>month</th>\n",
       "      <th>...</th>\n",
       "      <th>day_of_year_sin</th>\n",
       "      <th>day_of_year_cos</th>\n",
       "      <th>quarter_sin</th>\n",
       "      <th>quarter_cos</th>\n",
       "      <th>month_sin</th>\n",
       "      <th>month_cos</th>\n",
       "      <th>country_code</th>\n",
       "      <th>gdp</th>\n",
       "      <th>gdp_ratio</th>\n",
       "      <th>time_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Discount Stickers</td>\n",
       "      <td>Kaggle</td>\n",
       "      <td>973.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017213</td>\n",
       "      <td>0.999852</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>CAN</td>\n",
       "      <td>47560.666601</td>\n",
       "      <td>0.178301</td>\n",
       "      <td>Canada_Kaggle_Discount Stickers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Discount Stickers</td>\n",
       "      <td>Kaggle Tiers</td>\n",
       "      <td>906.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017213</td>\n",
       "      <td>0.999852</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>CAN</td>\n",
       "      <td>47560.666601</td>\n",
       "      <td>0.178301</td>\n",
       "      <td>Canada_Kaggle Tiers_Discount Stickers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Discount Stickers</td>\n",
       "      <td>Kerneler</td>\n",
       "      <td>423.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017213</td>\n",
       "      <td>0.999852</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>CAN</td>\n",
       "      <td>47560.666601</td>\n",
       "      <td>0.178301</td>\n",
       "      <td>Canada_Kerneler_Discount Stickers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Discount Stickers</td>\n",
       "      <td>Kerneler Dark Mode</td>\n",
       "      <td>491.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017213</td>\n",
       "      <td>0.999852</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>CAN</td>\n",
       "      <td>47560.666601</td>\n",
       "      <td>0.178301</td>\n",
       "      <td>Canada_Kerneler Dark Mode_Discount Stickers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Stickers for Less</td>\n",
       "      <td>Holographic Goose</td>\n",
       "      <td>300.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017213</td>\n",
       "      <td>0.999852</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>CAN</td>\n",
       "      <td>47560.666601</td>\n",
       "      <td>0.178301</td>\n",
       "      <td>Canada_Holographic Goose_Stickers for Less</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230125</th>\n",
       "      <td>230125</td>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>Premium Sticker Mart</td>\n",
       "      <td>Holographic Goose</td>\n",
       "      <td>466.0</td>\n",
       "      <td>2016</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017213</td>\n",
       "      <td>0.999852</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>SGP</td>\n",
       "      <td>56899.918181</td>\n",
       "      <td>0.231100</td>\n",
       "      <td>Singapore_Holographic Goose_Premium Sticker Mart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230126</th>\n",
       "      <td>230126</td>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>Premium Sticker Mart</td>\n",
       "      <td>Kaggle</td>\n",
       "      <td>2907.0</td>\n",
       "      <td>2016</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017213</td>\n",
       "      <td>0.999852</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>SGP</td>\n",
       "      <td>56899.918181</td>\n",
       "      <td>0.231100</td>\n",
       "      <td>Singapore_Kaggle_Premium Sticker Mart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230127</th>\n",
       "      <td>230127</td>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>Premium Sticker Mart</td>\n",
       "      <td>Kaggle Tiers</td>\n",
       "      <td>2299.0</td>\n",
       "      <td>2016</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017213</td>\n",
       "      <td>0.999852</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>SGP</td>\n",
       "      <td>56899.918181</td>\n",
       "      <td>0.231100</td>\n",
       "      <td>Singapore_Kaggle Tiers_Premium Sticker Mart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230128</th>\n",
       "      <td>230128</td>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>Premium Sticker Mart</td>\n",
       "      <td>Kerneler</td>\n",
       "      <td>1242.0</td>\n",
       "      <td>2016</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017213</td>\n",
       "      <td>0.999852</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>SGP</td>\n",
       "      <td>56899.918181</td>\n",
       "      <td>0.231100</td>\n",
       "      <td>Singapore_Kerneler_Premium Sticker Mart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230129</th>\n",
       "      <td>230129</td>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>Premium Sticker Mart</td>\n",
       "      <td>Kerneler Dark Mode</td>\n",
       "      <td>1622.0</td>\n",
       "      <td>2016</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017213</td>\n",
       "      <td>0.999852</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>SGP</td>\n",
       "      <td>56899.918181</td>\n",
       "      <td>0.231100</td>\n",
       "      <td>Singapore_Kerneler Dark Mode_Premium Sticker Mart</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>221259 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id       date    country                 store  \\\n",
       "1            1 2010-01-01     Canada     Discount Stickers   \n",
       "2            2 2010-01-01     Canada     Discount Stickers   \n",
       "3            3 2010-01-01     Canada     Discount Stickers   \n",
       "4            4 2010-01-01     Canada     Discount Stickers   \n",
       "5            5 2010-01-01     Canada     Stickers for Less   \n",
       "...        ...        ...        ...                   ...   \n",
       "230125  230125 2016-12-31  Singapore  Premium Sticker Mart   \n",
       "230126  230126 2016-12-31  Singapore  Premium Sticker Mart   \n",
       "230127  230127 2016-12-31  Singapore  Premium Sticker Mart   \n",
       "230128  230128 2016-12-31  Singapore  Premium Sticker Mart   \n",
       "230129  230129 2016-12-31  Singapore  Premium Sticker Mart   \n",
       "\n",
       "                   product  num_sold  year  day_of_month  days_in_month  \\\n",
       "1                   Kaggle     973.0  2010             1             31   \n",
       "2             Kaggle Tiers     906.0  2010             1             31   \n",
       "3                 Kerneler     423.0  2010             1             31   \n",
       "4       Kerneler Dark Mode     491.0  2010             1             31   \n",
       "5        Holographic Goose     300.0  2010             1             31   \n",
       "...                    ...       ...   ...           ...            ...   \n",
       "230125   Holographic Goose     466.0  2016            31             31   \n",
       "230126              Kaggle    2907.0  2016            31             31   \n",
       "230127        Kaggle Tiers    2299.0  2016            31             31   \n",
       "230128            Kerneler    1242.0  2016            31             31   \n",
       "230129  Kerneler Dark Mode    1622.0  2016            31             31   \n",
       "\n",
       "        month  ...  day_of_year_sin  day_of_year_cos   quarter_sin  \\\n",
       "1           1  ...         0.017213         0.999852  1.000000e+00   \n",
       "2           1  ...         0.017213         0.999852  1.000000e+00   \n",
       "3           1  ...         0.017213         0.999852  1.000000e+00   \n",
       "4           1  ...         0.017213         0.999852  1.000000e+00   \n",
       "5           1  ...         0.017213         0.999852  1.000000e+00   \n",
       "...       ...  ...              ...              ...           ...   \n",
       "230125     12  ...         0.017213         0.999852 -2.449294e-16   \n",
       "230126     12  ...         0.017213         0.999852 -2.449294e-16   \n",
       "230127     12  ...         0.017213         0.999852 -2.449294e-16   \n",
       "230128     12  ...         0.017213         0.999852 -2.449294e-16   \n",
       "230129     12  ...         0.017213         0.999852 -2.449294e-16   \n",
       "\n",
       "         quarter_cos     month_sin  month_cos  country_code           gdp  \\\n",
       "1       6.123234e-17  5.000000e-01   0.866025           CAN  47560.666601   \n",
       "2       6.123234e-17  5.000000e-01   0.866025           CAN  47560.666601   \n",
       "3       6.123234e-17  5.000000e-01   0.866025           CAN  47560.666601   \n",
       "4       6.123234e-17  5.000000e-01   0.866025           CAN  47560.666601   \n",
       "5       6.123234e-17  5.000000e-01   0.866025           CAN  47560.666601   \n",
       "...              ...           ...        ...           ...           ...   \n",
       "230125  1.000000e+00 -2.449294e-16   1.000000           SGP  56899.918181   \n",
       "230126  1.000000e+00 -2.449294e-16   1.000000           SGP  56899.918181   \n",
       "230127  1.000000e+00 -2.449294e-16   1.000000           SGP  56899.918181   \n",
       "230128  1.000000e+00 -2.449294e-16   1.000000           SGP  56899.918181   \n",
       "230129  1.000000e+00 -2.449294e-16   1.000000           SGP  56899.918181   \n",
       "\n",
       "        gdp_ratio                                            time_id  \n",
       "1        0.178301                    Canada_Kaggle_Discount Stickers  \n",
       "2        0.178301              Canada_Kaggle Tiers_Discount Stickers  \n",
       "3        0.178301                  Canada_Kerneler_Discount Stickers  \n",
       "4        0.178301        Canada_Kerneler Dark Mode_Discount Stickers  \n",
       "5        0.178301         Canada_Holographic Goose_Stickers for Less  \n",
       "...           ...                                                ...  \n",
       "230125   0.231100   Singapore_Holographic Goose_Premium Sticker Mart  \n",
       "230126   0.231100              Singapore_Kaggle_Premium Sticker Mart  \n",
       "230127   0.231100        Singapore_Kaggle Tiers_Premium Sticker Mart  \n",
       "230128   0.231100            Singapore_Kerneler_Premium Sticker Mart  \n",
       "230129   0.231100  Singapore_Kerneler Dark Mode_Premium Sticker Mart  \n",
       "\n",
       "[221259 rows x 39 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad909627",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-25T11:16:25.021436Z",
     "iopub.status.busy": "2025-01-25T11:16:25.021193Z",
     "iopub.status.idle": "2025-01-25T11:16:25.056603Z",
     "shell.execute_reply": "2025-01-25T11:16:25.055686Z"
    },
    "papermill": {
     "duration": 0.042189,
     "end_time": "2025-01-25T11:16:25.058176",
     "exception": false,
     "start_time": "2025-01-25T11:16:25.015987",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "target = 'num_sold'\n",
    "features = [col for col in train.columns if (col!='num_sold' and col!='date' and col!='id')]\n",
    "cols_to_drop = ['date', 'id']\n",
    "train.drop(columns=cols_to_drop, inplace=True)\n",
    "test.drop(columns=cols_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fee3f3ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-25T11:16:25.068512Z",
     "iopub.status.busy": "2025-01-25T11:16:25.068271Z",
     "iopub.status.idle": "2025-01-25T11:16:26.804139Z",
     "shell.execute_reply": "2025-01-25T11:16:26.803193Z"
    },
    "papermill": {
     "duration": 1.742737,
     "end_time": "2025-01-25T11:16:26.805758",
     "exception": false,
     "start_time": "2025-01-25T11:16:25.063021",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.stats import boxcox\n",
    "train['num_sold'], lmbda = boxcox(train['num_sold'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "523e609b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-25T11:16:26.817587Z",
     "iopub.status.busy": "2025-01-25T11:16:26.817107Z",
     "iopub.status.idle": "2025-01-25T11:16:27.885412Z",
     "shell.execute_reply": "2025-01-25T11:16:27.884485Z"
    },
    "papermill": {
     "duration": 1.075191,
     "end_time": "2025-01-25T11:16:27.886755",
     "exception": false,
     "start_time": "2025-01-25T11:16:26.811564",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20250125_111627\"\n"
     ]
    }
   ],
   "source": [
    "from autogluon.tabular import TabularPredictor\n",
    "predictor = TabularPredictor(label=target, eval_metric='mean_absolute_percentage_error', problem_type='regression')\n",
    "\n",
    "# tabular_df = TimeSeriesDataFrame(train)\n",
    "# label = 'num_sold'\n",
    "# features = [col for col in tabular_df.columns if col!='num_sold']\n",
    "# aml = TimeSeriesPredictor(label=label, eval_metric='mean_absolute_percentage_error', problem_type='regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53f79a8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-25T11:16:27.897650Z",
     "iopub.status.busy": "2025-01-25T11:16:27.897295Z",
     "iopub.status.idle": "2025-01-25T11:16:27.901644Z",
     "shell.execute_reply": "2025-01-25T11:16:27.901007Z"
    },
    "papermill": {
     "duration": 0.01093,
     "end_time": "2025-01-25T11:16:27.902802",
     "exception": false,
     "start_time": "2025-01-25T11:16:27.891872",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97a62d6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-25T11:16:27.913059Z",
     "iopub.status.busy": "2025-01-25T11:16:27.912835Z",
     "iopub.status.idle": "2025-01-25T12:12:32.405293Z",
     "shell.execute_reply": "2025-01-25T12:12:32.404443Z"
    },
    "papermill": {
     "duration": 3364.498976,
     "end_time": "2025-01-25T12:12:32.406611",
     "exception": false,
     "start_time": "2025-01-25T11:16:27.907635",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.2\n",
      "Python Version:     3.10.12\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP PREEMPT_DYNAMIC Sun Nov 10 10:07:59 UTC 2024\n",
      "CPU Count:          4\n",
      "Memory Avail:       30.04 GB / 31.35 GB (95.8%)\n",
      "Disk Space Avail:   19.50 GB / 19.52 GB (99.9%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=10, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 9900s of the 39600s of remaining time (25%).\n",
      "/usr/local/lib/python3.10/dist-packages/autogluon/tabular/predictor/predictor.py:1380: UserWarning: Failed to use ray for memory safe fits. Falling back to normal fit. Error: ValueError('ray==2.40.0 detected. 2.10.0 <= ray < 2.40.0 is required. You can use pip to install certain version of ray `pip install \"ray>=2.10.0,<2.40.0\"`')\n",
      "  stacked_overfitting = self._sub_fit_memory_save_wrapper(\n",
      "\t\tContext path: \"/kaggle/working/AutogluonModels/ag-20250125_111627/ds_sub_fit/sub_fit_ho\"\n",
      "Running DyStack sub-fit ...\n",
      "Beginning AutoGluon training ... Time limit = 9899s\n",
      "AutoGluon will save models to \"/kaggle/working/AutogluonModels/ag-20250125_111627/ds_sub_fit/sub_fit_ho\"\n",
      "Train Data Rows:    196674\n",
      "Train Data Columns: 36\n",
      "Label Column:       num_sold\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    30717.24 MB\n",
      "\tTrain Data (Original)  Memory Usage: 106.67 MB (0.3% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 7 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\tFitting TextSpecialFeatureGenerator...\n",
      "\t\t\tFitting BinnedFeatureGenerator...\n",
      "\t\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\t\tFitting TextNgramFeatureGenerator...\n",
      "\t\t\tFitting CountVectorizer for text features: ['time_id']\n",
      "\t\t\tCountVectorizer fit with vocabulary size = 6\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUnused Original Features (Count: 3): ['weekday', 'daysinmonth', 'country_code']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', [])    : 2 | ['weekday', 'daysinmonth']\n",
      "\t\t('object', []) : 1 | ['country_code']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])         :  6 | ['is_month_start', 'is_month_end', 'is_quarter_start', 'is_year_start', 'is_year_end', ...]\n",
      "\t\t('float', [])        : 14 | ['day_of_week_sin', 'day_of_week_cos', 'week_of_year_sin', 'week_of_year_cos', 'day_of_month_sin', ...]\n",
      "\t\t('int', [])          :  9 | ['year', 'day_of_month', 'days_in_month', 'month', 'day_of_week', ...]\n",
      "\t\t('object', [])       :  3 | ['country', 'store', 'product']\n",
      "\t\t('object', ['text']) :  1 | ['time_id']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])                    :  3 | ['country', 'store', 'product']\n",
      "\t\t('category', ['text_as_category'])  :  1 | ['time_id']\n",
      "\t\t('float', [])                       : 14 | ['day_of_week_sin', 'day_of_week_cos', 'week_of_year_sin', 'week_of_year_cos', 'day_of_month_sin', ...]\n",
      "\t\t('int', [])                         :  8 | ['year', 'day_of_month', 'days_in_month', 'month', 'day_of_week', ...]\n",
      "\t\t('int', ['binned', 'text_special']) :  5 | ['time_id.char_count', 'time_id.word_count', 'time_id.capital_ratio', 'time_id.lower_ratio', 'time_id.symbol_ratio. ']\n",
      "\t\t('int', ['bool'])                   :  7 | ['is_month_start', 'is_month_end', 'is_quarter_start', 'is_year_start', 'is_year_end', ...]\n",
      "\t\t('int', ['text_ngram'])             :  3 | ['__nlp__.for', '__nlp__.mart', '__nlp__._total_']\n",
      "\t7.7s = Fit runtime\n",
      "\t33 features in original data used to generate 41 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 37.14 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 7.86s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_percentage_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'GBM': [{'num_boost_round': 10000, 'tree_method': 'gpu_hist', 'device': 'gpu'}],\n",
      "\t'CAT': [{'task_type': 'GPU'}],\n",
      "\t'XGB': [{'tree_method': 'gpu_hist', 'gpu_id': 0}],\n",
      "\t'NN_TORCH': [{'use_gpu': True}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 4 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 6592.64s of the 9891.43s of remaining time.\n",
      "Will use sequential fold fitting strategy because import of ray failed. Reason: ray==2.40.0 detected. 2.10.0 <= ray < 2.40.0 is required. You can use pip to install certain version of ray `pip install \"ray>=2.10.0,<2.40.0\"`\n",
      "\tFitting 10 child models (S1F1 - S1F10) | Fitting with SequentialLocalFoldFittingStrategy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 0.346445\tvalid_set's mean_absolute_percentage_error: -0.019409\n",
      "[2000]\tvalid_set's l2: 0.332618\tvalid_set's mean_absolute_percentage_error: -0.018991\n",
      "[3000]\tvalid_set's l2: 0.329886\tvalid_set's mean_absolute_percentage_error: -0.0189074\n",
      "[4000]\tvalid_set's l2: 0.329645\tvalid_set's mean_absolute_percentage_error: -0.0188838\n",
      "[5000]\tvalid_set's l2: 0.33064\tvalid_set's mean_absolute_percentage_error: -0.0189004\n",
      "[1000]\tvalid_set's l2: 0.338388\tvalid_set's mean_absolute_percentage_error: -0.0194858\n",
      "[2000]\tvalid_set's l2: 0.32498\tvalid_set's mean_absolute_percentage_error: -0.0191338\n",
      "[3000]\tvalid_set's l2: 0.322217\tvalid_set's mean_absolute_percentage_error: -0.0190162\n",
      "[4000]\tvalid_set's l2: 0.321595\tvalid_set's mean_absolute_percentage_error: -0.018966\n",
      "[5000]\tvalid_set's l2: 0.322302\tvalid_set's mean_absolute_percentage_error: -0.0189651\n",
      "[1000]\tvalid_set's l2: 0.342949\tvalid_set's mean_absolute_percentage_error: -0.0194918\n",
      "[2000]\tvalid_set's l2: 0.329122\tvalid_set's mean_absolute_percentage_error: -0.0190839\n",
      "[3000]\tvalid_set's l2: 0.326749\tvalid_set's mean_absolute_percentage_error: -0.0189895\n",
      "[4000]\tvalid_set's l2: 0.32687\tvalid_set's mean_absolute_percentage_error: -0.0189472\n",
      "[5000]\tvalid_set's l2: 0.32772\tvalid_set's mean_absolute_percentage_error: -0.0189475\n",
      "[1000]\tvalid_set's l2: 0.340394\tvalid_set's mean_absolute_percentage_error: -0.019454\n",
      "[2000]\tvalid_set's l2: 0.329465\tvalid_set's mean_absolute_percentage_error: -0.0190921\n",
      "[3000]\tvalid_set's l2: 0.326247\tvalid_set's mean_absolute_percentage_error: -0.01896\n",
      "[4000]\tvalid_set's l2: 0.326339\tvalid_set's mean_absolute_percentage_error: -0.0189194\n",
      "[5000]\tvalid_set's l2: 0.327025\tvalid_set's mean_absolute_percentage_error: -0.0189102\n",
      "[6000]\tvalid_set's l2: 0.328006\tvalid_set's mean_absolute_percentage_error: -0.0189249\n",
      "[1000]\tvalid_set's l2: 0.336421\tvalid_set's mean_absolute_percentage_error: -0.0193888\n",
      "[2000]\tvalid_set's l2: 0.325857\tvalid_set's mean_absolute_percentage_error: -0.0190926\n",
      "[3000]\tvalid_set's l2: 0.323792\tvalid_set's mean_absolute_percentage_error: -0.0189936\n",
      "[4000]\tvalid_set's l2: 0.324052\tvalid_set's mean_absolute_percentage_error: -0.0189788\n",
      "[5000]\tvalid_set's l2: 0.324925\tvalid_set's mean_absolute_percentage_error: -0.0189812\n",
      "[1000]\tvalid_set's l2: 0.339704\tvalid_set's mean_absolute_percentage_error: -0.0193715\n",
      "[2000]\tvalid_set's l2: 0.328591\tvalid_set's mean_absolute_percentage_error: -0.0190046\n",
      "[3000]\tvalid_set's l2: 0.324709\tvalid_set's mean_absolute_percentage_error: -0.0188482\n",
      "[4000]\tvalid_set's l2: 0.323991\tvalid_set's mean_absolute_percentage_error: -0.0188201\n",
      "[5000]\tvalid_set's l2: 0.325371\tvalid_set's mean_absolute_percentage_error: -0.0188338\n",
      "[1000]\tvalid_set's l2: 0.339533\tvalid_set's mean_absolute_percentage_error: -0.0194625\n",
      "[2000]\tvalid_set's l2: 0.328425\tvalid_set's mean_absolute_percentage_error: -0.0191241\n",
      "[3000]\tvalid_set's l2: 0.32601\tvalid_set's mean_absolute_percentage_error: -0.0190138\n",
      "[4000]\tvalid_set's l2: 0.326395\tvalid_set's mean_absolute_percentage_error: -0.019004\n",
      "[1000]\tvalid_set's l2: 0.336827\tvalid_set's mean_absolute_percentage_error: -0.0194501\n",
      "[2000]\tvalid_set's l2: 0.323717\tvalid_set's mean_absolute_percentage_error: -0.0190519\n",
      "[3000]\tvalid_set's l2: 0.322624\tvalid_set's mean_absolute_percentage_error: -0.0189977\n",
      "[4000]\tvalid_set's l2: 0.323485\tvalid_set's mean_absolute_percentage_error: -0.0189927\n",
      "[1000]\tvalid_set's l2: 0.339164\tvalid_set's mean_absolute_percentage_error: -0.0192792\n",
      "[2000]\tvalid_set's l2: 0.326746\tvalid_set's mean_absolute_percentage_error: -0.0189074\n",
      "[3000]\tvalid_set's l2: 0.32672\tvalid_set's mean_absolute_percentage_error: -0.018887\n",
      "[4000]\tvalid_set's l2: 0.327108\tvalid_set's mean_absolute_percentage_error: -0.0188883\n",
      "[5000]\tvalid_set's l2: 0.328222\tvalid_set's mean_absolute_percentage_error: -0.0189091\n",
      "[1000]\tvalid_set's l2: 0.337855\tvalid_set's mean_absolute_percentage_error: -0.0194509\n",
      "[2000]\tvalid_set's l2: 0.324218\tvalid_set's mean_absolute_percentage_error: -0.0190513\n",
      "[3000]\tvalid_set's l2: 0.322696\tvalid_set's mean_absolute_percentage_error: -0.0189995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0189\t = Validation score   (-mean_absolute_percentage_error)\n",
      "\t337.81s\t = Training   runtime\n",
      "\t17.63s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 6234.41s of the 9533.20s of remaining time.\n",
      "\tFitting 10 child models (S1F1 - S1F10) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-0.0244\t = Validation score   (-mean_absolute_percentage_error)\n",
      "\t685.09s\t = Training   runtime\n",
      "\t2.91s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 5545.95s of the 8844.74s of remaining time.\n",
      "\tFitting 10 child models (S1F1 - S1F10) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [11:34:05] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [11:34:05] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [11:34:14] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [11:34:14] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [11:34:16] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [11:34:26] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [11:34:28] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [11:34:38] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [11:34:40] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [11:34:49] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [11:34:51] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [11:35:01] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [11:35:03] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [11:35:13] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [11:35:15] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [11:35:23] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [11:35:25] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [11:35:36] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [11:35:38] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [11:35:47] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [11:35:49] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [11:36:01] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\t-0.0191\t = Validation score   (-mean_absolute_percentage_error)\n",
      "\t114.76s\t = Training   runtime\n",
      "\t2.37s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 5428.15s of the 8726.94s of remaining time.\n",
      "\tFitting 10 child models (S1F1 - S1F10) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetTorch_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tEmbedNet._set_params() got an unexpected keyword argument 'use_gpu'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 317, in after_all_folds_scheduled\n",
      "    self._fit_fold_model(job)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 322, in _fit_fold_model\n",
      "    fold_model = self._fit(self.model_base, time_start_fold, time_limit_fold, fold_ctx, self.model_base_kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 358, in _fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, num_cpus=num_cpus, num_gpus=num_gpus, **kwargs_fold)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 214, in _fit\n",
      "    self._get_net(train_dataset, params=params)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 244, in _get_net\n",
      "    self.model = EmbedNet(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/tabular_nn/torch/torch_network_modules.py\", line 28, in __init__\n",
      "    params = self._set_params(**kwargs)\n",
      "TypeError: EmbedNet._set_params() got an unexpected keyword argument 'use_gpu'\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 659.26s of the 8721.97s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 0.6, 'XGBoost_BAG_L1': 0.4}\n",
      "\t-0.0187\t = Validation score   (-mean_absolute_percentage_error)\n",
      "\t0.15s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 4 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 8721.79s of the 8721.78s of remaining time.\n",
      "\tFitting 10 child models (S1F1 - S1F10) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-0.0188\t = Validation score   (-mean_absolute_percentage_error)\n",
      "\t37.72s\t = Training   runtime\n",
      "\t1.19s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 8682.49s of the 8682.48s of remaining time.\n",
      "\tFitting 10 child models (S1F1 - S1F10) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-0.0197\t = Validation score   (-mean_absolute_percentage_error)\n",
      "\t229.85s\t = Training   runtime\n",
      "\t0.57s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 8451.72s of the 8451.71s of remaining time.\n",
      "\tFitting 10 child models (S1F1 - S1F10) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [11:40:38] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [11:40:39] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [11:40:41] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [11:40:41] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [11:40:43] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [11:40:44] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [11:40:46] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [11:40:47] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [11:40:48] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [11:40:49] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [11:40:51] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [11:40:52] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [11:40:54] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [11:40:55] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [11:40:56] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [11:40:57] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [11:40:59] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [11:41:00] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [11:41:02] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [11:41:03] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\t-0.0188\t = Validation score   (-mean_absolute_percentage_error)\n",
      "\t25.44s\t = Training   runtime\n",
      "\t0.67s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 8425.29s of the 8425.27s of remaining time.\n",
      "\tFitting 10 child models (S1F1 - S1F10) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetTorch_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tEmbedNet._set_params() got an unexpected keyword argument 'use_gpu'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 317, in after_all_folds_scheduled\n",
      "    self._fit_fold_model(job)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 322, in _fit_fold_model\n",
      "    fold_model = self._fit(self.model_base, time_start_fold, time_limit_fold, fold_ctx, self.model_base_kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 358, in _fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, num_cpus=num_cpus, num_gpus=num_gpus, **kwargs_fold)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 214, in _fit\n",
      "    self._get_net(train_dataset, params=params)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 244, in _get_net\n",
      "    self.model = EmbedNet(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/tabular_nn/torch/torch_network_modules.py\", line 28, in __init__\n",
      "    params = self._set_params(**kwargs)\n",
      "TypeError: EmbedNet._set_params() got an unexpected keyword argument 'use_gpu'\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 872.18s of the 8423.01s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 0.312, 'XGBoost_BAG_L2': 0.25, 'XGBoost_BAG_L1': 0.188, 'LightGBM_BAG_L2': 0.188, 'CatBoost_BAG_L2': 0.062}\n",
      "\t-0.0187\t = Validation score   (-mean_absolute_percentage_error)\n",
      "\t0.28s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 1476.66s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 775.6 rows/s (19668 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/kaggle/working/AutogluonModels/ag-20250125_111627/ds_sub_fit/sub_fit_ho\")\n",
      "Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                 model  score_holdout  score_val                     eval_metric  pred_time_test  pred_time_val     fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0  WeightedEnsemble_L3      -0.018649  -0.018668  mean_absolute_percentage_error       66.609408      25.360317  1430.959472                 0.003525                0.003057           0.281251            3       True          8\n",
      "1  WeightedEnsemble_L2      -0.018724  -0.018721  mean_absolute_percentage_error       60.477931      20.006796   452.721227                 0.002384                0.003007           0.153878            2       True          4\n",
      "2       XGBoost_BAG_L2      -0.018730  -0.018794  mean_absolute_percentage_error       64.596612      23.590449  1163.100051                 1.563046                0.672611          25.443745            2       True          7\n",
      "3      LightGBM_BAG_L2      -0.018734  -0.018843  mean_absolute_percentage_error       64.600780      24.112150  1175.380071                 1.567214                1.194312          37.723765            2       True          5\n",
      "4       XGBoost_BAG_L1      -0.018861  -0.019121  mean_absolute_percentage_error       22.889826       2.370489   114.761788                22.889826                2.370489         114.761788            1       True          3\n",
      "5      LightGBM_BAG_L1      -0.018895  -0.018930  mean_absolute_percentage_error       37.585720      17.633300   337.805561                37.585720               17.633300         337.805561            1       True          1\n",
      "6      CatBoost_BAG_L2      -0.019248  -0.019700  mean_absolute_percentage_error       63.475624      23.490336  1367.510711                 0.442057                0.572499         229.854405            2       True          6\n",
      "7      CatBoost_BAG_L1      -0.020375  -0.024373  mean_absolute_percentage_error        2.558021       2.914049   685.088957                 2.558021                2.914049         685.088957            1       True          2\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t1545s\t = DyStack   runtime |\t38055s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 38055s\n",
      "AutoGluon will save models to \"/kaggle/working/AutogluonModels/ag-20250125_111627\"\n",
      "Train Data Rows:    221259\n",
      "Train Data Columns: 36\n",
      "Label Column:       num_sold\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    29756.49 MB\n",
      "\tTrain Data (Original)  Memory Usage: 120.01 MB (0.4% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 7 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\tFitting TextSpecialFeatureGenerator...\n",
      "\t\t\tFitting BinnedFeatureGenerator...\n",
      "\t\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\t\tFitting TextNgramFeatureGenerator...\n",
      "\t\t\tFitting CountVectorizer for text features: ['time_id']\n",
      "\t\t\tCountVectorizer fit with vocabulary size = 6\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUnused Original Features (Count: 3): ['weekday', 'daysinmonth', 'country_code']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', [])    : 2 | ['weekday', 'daysinmonth']\n",
      "\t\t('object', []) : 1 | ['country_code']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('bool', [])         :  6 | ['is_month_start', 'is_month_end', 'is_quarter_start', 'is_year_start', 'is_year_end', ...]\n",
      "\t\t('float', [])        : 14 | ['day_of_week_sin', 'day_of_week_cos', 'week_of_year_sin', 'week_of_year_cos', 'day_of_month_sin', ...]\n",
      "\t\t('int', [])          :  9 | ['year', 'day_of_month', 'days_in_month', 'month', 'day_of_week', ...]\n",
      "\t\t('object', [])       :  3 | ['country', 'store', 'product']\n",
      "\t\t('object', ['text']) :  1 | ['time_id']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])                    :  3 | ['country', 'store', 'product']\n",
      "\t\t('category', ['text_as_category'])  :  1 | ['time_id']\n",
      "\t\t('float', [])                       : 14 | ['day_of_week_sin', 'day_of_week_cos', 'week_of_year_sin', 'week_of_year_cos', 'day_of_month_sin', ...]\n",
      "\t\t('int', [])                         :  8 | ['year', 'day_of_month', 'days_in_month', 'month', 'day_of_week', ...]\n",
      "\t\t('int', ['binned', 'text_special']) :  5 | ['time_id.char_count', 'time_id.word_count', 'time_id.capital_ratio', 'time_id.lower_ratio', 'time_id.symbol_ratio. ']\n",
      "\t\t('int', ['bool'])                   :  7 | ['is_month_start', 'is_month_end', 'is_quarter_start', 'is_year_start', 'is_year_end', ...]\n",
      "\t\t('int', ['text_ngram'])             :  3 | ['__nlp__.for', '__nlp__.mart', '__nlp__._total_']\n",
      "\t8.2s = Fit runtime\n",
      "\t33 features in original data used to generate 41 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 41.78 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 8.31s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_percentage_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'GBM': [{'num_boost_round': 10000, 'tree_method': 'gpu_hist', 'device': 'gpu'}],\n",
      "\t'CAT': [{'task_type': 'GPU'}],\n",
      "\t'XGB': [{'tree_method': 'gpu_hist', 'gpu_id': 0}],\n",
      "\t'NN_TORCH': [{'use_gpu': True}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 4 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 25358.18s of the 38046.78s of remaining time.\n",
      "\tFitting 10 child models (S1F1 - S1F10) | Fitting with SequentialLocalFoldFittingStrategy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 0.336724\tvalid_set's mean_absolute_percentage_error: -0.0193734\n",
      "[2000]\tvalid_set's l2: 0.327188\tvalid_set's mean_absolute_percentage_error: -0.0190324\n",
      "[3000]\tvalid_set's l2: 0.325145\tvalid_set's mean_absolute_percentage_error: -0.0189589\n",
      "[4000]\tvalid_set's l2: 0.325475\tvalid_set's mean_absolute_percentage_error: -0.0189526\n",
      "[5000]\tvalid_set's l2: 0.326644\tvalid_set's mean_absolute_percentage_error: -0.0189539\n",
      "[6000]\tvalid_set's l2: 0.327797\tvalid_set's mean_absolute_percentage_error: -0.0189673\n",
      "[1000]\tvalid_set's l2: 0.34376\tvalid_set's mean_absolute_percentage_error: -0.0193683\n",
      "[2000]\tvalid_set's l2: 0.32985\tvalid_set's mean_absolute_percentage_error: -0.0189505\n",
      "[3000]\tvalid_set's l2: 0.328471\tvalid_set's mean_absolute_percentage_error: -0.0188648\n",
      "[4000]\tvalid_set's l2: 0.32927\tvalid_set's mean_absolute_percentage_error: -0.0188639\n",
      "[1000]\tvalid_set's l2: 0.336269\tvalid_set's mean_absolute_percentage_error: -0.0194571\n",
      "[2000]\tvalid_set's l2: 0.323529\tvalid_set's mean_absolute_percentage_error: -0.0190545\n",
      "[3000]\tvalid_set's l2: 0.321019\tvalid_set's mean_absolute_percentage_error: -0.0189342\n",
      "[4000]\tvalid_set's l2: 0.321214\tvalid_set's mean_absolute_percentage_error: -0.0189066\n",
      "[5000]\tvalid_set's l2: 0.321886\tvalid_set's mean_absolute_percentage_error: -0.0189111\n",
      "[1000]\tvalid_set's l2: 0.34319\tvalid_set's mean_absolute_percentage_error: -0.0194692\n",
      "[2000]\tvalid_set's l2: 0.327078\tvalid_set's mean_absolute_percentage_error: -0.0189638\n",
      "[3000]\tvalid_set's l2: 0.326016\tvalid_set's mean_absolute_percentage_error: -0.0189032\n",
      "[4000]\tvalid_set's l2: 0.325413\tvalid_set's mean_absolute_percentage_error: -0.0188697\n",
      "[5000]\tvalid_set's l2: 0.326367\tvalid_set's mean_absolute_percentage_error: -0.0188791\n",
      "[1000]\tvalid_set's l2: 0.342202\tvalid_set's mean_absolute_percentage_error: -0.0191944\n",
      "[2000]\tvalid_set's l2: 0.327308\tvalid_set's mean_absolute_percentage_error: -0.0187687\n",
      "[3000]\tvalid_set's l2: 0.325757\tvalid_set's mean_absolute_percentage_error: -0.0186882\n",
      "[4000]\tvalid_set's l2: 0.326002\tvalid_set's mean_absolute_percentage_error: -0.0186864\n",
      "[1000]\tvalid_set's l2: 0.341048\tvalid_set's mean_absolute_percentage_error: -0.0193679\n",
      "[2000]\tvalid_set's l2: 0.325278\tvalid_set's mean_absolute_percentage_error: -0.0189225\n",
      "[3000]\tvalid_set's l2: 0.322368\tvalid_set's mean_absolute_percentage_error: -0.018826\n",
      "[4000]\tvalid_set's l2: 0.32204\tvalid_set's mean_absolute_percentage_error: -0.0188158\n",
      "[5000]\tvalid_set's l2: 0.323334\tvalid_set's mean_absolute_percentage_error: -0.0188455\n",
      "[1000]\tvalid_set's l2: 0.341391\tvalid_set's mean_absolute_percentage_error: -0.0194869\n",
      "[2000]\tvalid_set's l2: 0.326185\tvalid_set's mean_absolute_percentage_error: -0.0190667\n",
      "[3000]\tvalid_set's l2: 0.324643\tvalid_set's mean_absolute_percentage_error: -0.0190032\n",
      "[4000]\tvalid_set's l2: 0.324794\tvalid_set's mean_absolute_percentage_error: -0.0189821\n",
      "[5000]\tvalid_set's l2: 0.32532\tvalid_set's mean_absolute_percentage_error: -0.0189628\n",
      "[1000]\tvalid_set's l2: 0.338284\tvalid_set's mean_absolute_percentage_error: -0.019468\n",
      "[2000]\tvalid_set's l2: 0.32545\tvalid_set's mean_absolute_percentage_error: -0.0191003\n",
      "[3000]\tvalid_set's l2: 0.3225\tvalid_set's mean_absolute_percentage_error: -0.0189887\n",
      "[4000]\tvalid_set's l2: 0.322368\tvalid_set's mean_absolute_percentage_error: -0.0189658\n",
      "[5000]\tvalid_set's l2: 0.32361\tvalid_set's mean_absolute_percentage_error: -0.0189941\n",
      "[1000]\tvalid_set's l2: 0.332097\tvalid_set's mean_absolute_percentage_error: -0.0193294\n",
      "[2000]\tvalid_set's l2: 0.319589\tvalid_set's mean_absolute_percentage_error: -0.0189659\n",
      "[3000]\tvalid_set's l2: 0.318236\tvalid_set's mean_absolute_percentage_error: -0.0189168\n",
      "[4000]\tvalid_set's l2: 0.318785\tvalid_set's mean_absolute_percentage_error: -0.018917\n",
      "[1000]\tvalid_set's l2: 0.331679\tvalid_set's mean_absolute_percentage_error: -0.0193366\n",
      "[2000]\tvalid_set's l2: 0.322945\tvalid_set's mean_absolute_percentage_error: -0.0190514\n",
      "[3000]\tvalid_set's l2: 0.320086\tvalid_set's mean_absolute_percentage_error: -0.0189395\n",
      "[4000]\tvalid_set's l2: 0.319524\tvalid_set's mean_absolute_percentage_error: -0.0189038\n",
      "[5000]\tvalid_set's l2: 0.319687\tvalid_set's mean_absolute_percentage_error: -0.0188767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0189\t = Validation score   (-mean_absolute_percentage_error)\n",
      "\t312.93s\t = Training   runtime\n",
      "\t34.84s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 25007.54s of the 37696.14s of remaining time.\n",
      "\tFitting 10 child models (S1F1 - S1F10) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-0.0191\t = Validation score   (-mean_absolute_percentage_error)\n",
      "\t1075.34s\t = Training   runtime\n",
      "\t4.58s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 23927.05s of the 36615.65s of remaining time.\n",
      "\tFitting 10 child models (S1F1 - S1F10) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [12:06:14] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [12:06:26] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [12:06:28] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [12:06:40] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [12:06:42] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [12:06:51] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [12:06:53] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [12:07:07] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [12:07:09] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [12:07:21] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [12:07:23] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [12:07:35] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [12:07:37] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [12:07:49] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [12:07:51] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [12:08:02] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [12:08:03] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [12:08:15] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [12:08:17] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [12:08:27] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\t-0.019\t = Validation score   (-mean_absolute_percentage_error)\n",
      "\t131.87s\t = Training   runtime\n",
      "\t3.23s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 23791.20s of the 36479.80s of remaining time.\n",
      "\tFitting 10 child models (S1F1 - S1F10) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetTorch_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\tEmbedNet._set_params() got an unexpected keyword argument 'use_gpu'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 317, in after_all_folds_scheduled\n",
      "    self._fit_fold_model(job)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 322, in _fit_fold_model\n",
      "    fold_model = self._fit(self.model_base, time_start_fold, time_limit_fold, fold_ctx, self.model_base_kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 358, in _fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, num_cpus=num_cpus, num_gpus=num_gpus, **kwargs_fold)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 214, in _fit\n",
      "    self._get_net(train_dataset, params=params)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 244, in _get_net\n",
      "    self.model = EmbedNet(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/tabular_nn/torch/torch_network_modules.py\", line 28, in __init__\n",
      "    params = self._set_params(**kwargs)\n",
      "TypeError: EmbedNet._set_params() got an unexpected keyword argument 'use_gpu'\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 2535.82s of the 36477.44s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 0.462, 'CatBoost_BAG_L1': 0.308, 'XGBoost_BAG_L1': 0.231}\n",
      "\t-0.0186\t = Validation score   (-mean_absolute_percentage_error)\n",
      "\t0.18s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 4 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 36477.23s of the 36477.22s of remaining time.\n",
      "\tFitting 10 child models (S1F1 - S1F10) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\t-0.0186\t = Validation score   (-mean_absolute_percentage_error)\n",
      "\t35.91s\t = Training   runtime\n",
      "\t1.29s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 36439.64s of the 36439.62s of remaining time.\n",
      "\tFitting 10 child models (S1F1 - S1F10) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-0.0194\t = Validation score   (-mean_absolute_percentage_error)\n",
      "\t171.87s\t = Training   runtime\n",
      "\t0.47s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 36266.93s of the 36266.91s of remaining time.\n",
      "\tFitting 10 child models (S1F1 - S1F10) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [12:12:03] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [12:12:04] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [12:12:06] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [12:12:06] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [12:12:08] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [12:12:09] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [12:12:11] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [12:12:12] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [12:12:14] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [12:12:15] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [12:12:17] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [12:12:18] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [12:12:20] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [12:12:21] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [12:12:22] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [12:12:23] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [12:12:25] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [12:12:26] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [12:12:28] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [12:12:29] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\t-0.0186\t = Validation score   (-mean_absolute_percentage_error)\n",
      "\t27.02s\t = Training   runtime\n",
      "\t1.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 36238.57s of the 36238.55s of remaining time.\n",
      "\tFitting 10 child models (S1F1 - S1F10) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tWarning: Exception caused NeuralNetTorch_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\tEmbedNet._set_params() got an unexpected keyword argument 'use_gpu'\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\", line 2106, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/trainer/abstract_trainer.py\", line 1993, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 298, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 724, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 317, in after_all_folds_scheduled\n",
      "    self._fit_fold_model(job)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 322, in _fit_fold_model\n",
      "    fold_model = self._fit(self.model_base, time_start_fold, time_limit_fold, fold_ctx, self.model_base_kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 358, in _fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, num_cpus=num_cpus, num_gpus=num_gpus, **kwargs_fold)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 925, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 214, in _fit\n",
      "    self._get_net(train_dataset, params=params)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 244, in _get_net\n",
      "    self.model = EmbedNet(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/tabular_nn/torch/torch_network_modules.py\", line 28, in __init__\n",
      "    params = self._set_params(**kwargs)\n",
      "TypeError: EmbedNet._set_params() got an unexpected keyword argument 'use_gpu'\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 3647.72s of the 36235.99s of remaining time.\n",
      "\tEnsemble Weights: {'XGBoost_BAG_L2': 0.333, 'LightGBM_BAG_L2': 0.278, 'LightGBM_BAG_L1': 0.167, 'CatBoost_BAG_L1': 0.111, 'XGBoost_BAG_L1': 0.056, 'CatBoost_BAG_L2': 0.056}\n",
      "\t-0.0185\t = Validation score   (-mean_absolute_percentage_error)\n",
      "\t0.35s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 1819.55s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 487.3 rows/s (22126 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/kaggle/working/AutogluonModels/ag-20250125_111627\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<autogluon.tabular.predictor.predictor.TabularPredictor at 0x7d0331653370>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.fit(\n",
    "    train_data=train,\n",
    "    time_limit=39600,\n",
    "    presets='best_quality',\n",
    "    num_bag_folds=10,\n",
    "    hyperparameters={\n",
    "        'GBM': {'num_boost_round': 10000, 'tree_method': 'gpu_hist', 'device': 'gpu'},  # LightGBM\n",
    "        'CAT': {'task_type': 'GPU'},  # CatBoost\n",
    "        'XGB': {'tree_method': 'gpu_hist', 'gpu_id': 0},  # XGBoost\n",
    "        'NN_TORCH': {'use_gpu': True},  # Neural networks with GPU\n",
    "\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c219745a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-25T12:12:32.458716Z",
     "iopub.status.busy": "2025-01-25T12:12:32.458420Z",
     "iopub.status.idle": "2025-01-25T12:12:32.471592Z",
     "shell.execute_reply": "2025-01-25T12:12:32.470881Z"
    },
    "papermill": {
     "duration": 0.040587,
     "end_time": "2025-01-25T12:12:32.472872",
     "exception": false,
     "start_time": "2025-01-25T12:12:32.432285",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "leaderboard = predictor.leaderboard()\n",
    "leaderboard.to_csv('leaderboard.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e52eb6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-25T12:12:32.525600Z",
     "iopub.status.busy": "2025-01-25T12:12:32.525300Z",
     "iopub.status.idle": "2025-01-25T12:12:32.537388Z",
     "shell.execute_reply": "2025-01-25T12:12:32.536641Z"
    },
    "papermill": {
     "duration": 0.039253,
     "end_time": "2025-01-25T12:12:32.538529",
     "exception": false,
     "start_time": "2025-01-25T12:12:32.499276",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>score_val</th>\n",
       "      <th>eval_metric</th>\n",
       "      <th>pred_time_val</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>pred_time_val_marginal</th>\n",
       "      <th>fit_time_marginal</th>\n",
       "      <th>stack_level</th>\n",
       "      <th>can_infer</th>\n",
       "      <th>fit_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WeightedEnsemble_L3</td>\n",
       "      <td>-0.018492</td>\n",
       "      <td>mean_absolute_percentage_error</td>\n",
       "      <td>45.409579</td>\n",
       "      <td>1755.288345</td>\n",
       "      <td>0.004287</td>\n",
       "      <td>0.349548</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WeightedEnsemble_L2</td>\n",
       "      <td>-0.018581</td>\n",
       "      <td>mean_absolute_percentage_error</td>\n",
       "      <td>42.646244</td>\n",
       "      <td>1520.315774</td>\n",
       "      <td>0.003919</td>\n",
       "      <td>0.177280</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGBoost_BAG_L2</td>\n",
       "      <td>-0.018603</td>\n",
       "      <td>mean_absolute_percentage_error</td>\n",
       "      <td>43.642680</td>\n",
       "      <td>1547.157800</td>\n",
       "      <td>1.000355</td>\n",
       "      <td>27.019306</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LightGBM_BAG_L2</td>\n",
       "      <td>-0.018626</td>\n",
       "      <td>mean_absolute_percentage_error</td>\n",
       "      <td>43.930616</td>\n",
       "      <td>1556.049509</td>\n",
       "      <td>1.288291</td>\n",
       "      <td>35.911015</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LightGBM_BAG_L1</td>\n",
       "      <td>-0.018875</td>\n",
       "      <td>mean_absolute_percentage_error</td>\n",
       "      <td>34.835170</td>\n",
       "      <td>312.931865</td>\n",
       "      <td>34.835170</td>\n",
       "      <td>312.931865</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>XGBoost_BAG_L1</td>\n",
       "      <td>-0.019034</td>\n",
       "      <td>mean_absolute_percentage_error</td>\n",
       "      <td>3.229614</td>\n",
       "      <td>131.870789</td>\n",
       "      <td>3.229614</td>\n",
       "      <td>131.870789</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CatBoost_BAG_L1</td>\n",
       "      <td>-0.019088</td>\n",
       "      <td>mean_absolute_percentage_error</td>\n",
       "      <td>4.577541</td>\n",
       "      <td>1075.335840</td>\n",
       "      <td>4.577541</td>\n",
       "      <td>1075.335840</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CatBoost_BAG_L2</td>\n",
       "      <td>-0.019449</td>\n",
       "      <td>mean_absolute_percentage_error</td>\n",
       "      <td>43.116646</td>\n",
       "      <td>1692.008476</td>\n",
       "      <td>0.474321</td>\n",
       "      <td>171.869982</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model  score_val                     eval_metric  \\\n",
       "0  WeightedEnsemble_L3  -0.018492  mean_absolute_percentage_error   \n",
       "1  WeightedEnsemble_L2  -0.018581  mean_absolute_percentage_error   \n",
       "2       XGBoost_BAG_L2  -0.018603  mean_absolute_percentage_error   \n",
       "3      LightGBM_BAG_L2  -0.018626  mean_absolute_percentage_error   \n",
       "4      LightGBM_BAG_L1  -0.018875  mean_absolute_percentage_error   \n",
       "5       XGBoost_BAG_L1  -0.019034  mean_absolute_percentage_error   \n",
       "6      CatBoost_BAG_L1  -0.019088  mean_absolute_percentage_error   \n",
       "7      CatBoost_BAG_L2  -0.019449  mean_absolute_percentage_error   \n",
       "\n",
       "   pred_time_val     fit_time  pred_time_val_marginal  fit_time_marginal  \\\n",
       "0      45.409579  1755.288345                0.004287           0.349548   \n",
       "1      42.646244  1520.315774                0.003919           0.177280   \n",
       "2      43.642680  1547.157800                1.000355          27.019306   \n",
       "3      43.930616  1556.049509                1.288291          35.911015   \n",
       "4      34.835170   312.931865               34.835170         312.931865   \n",
       "5       3.229614   131.870789                3.229614         131.870789   \n",
       "6       4.577541  1075.335840                4.577541        1075.335840   \n",
       "7      43.116646  1692.008476                0.474321         171.869982   \n",
       "\n",
       "   stack_level  can_infer  fit_order  \n",
       "0            3       True          8  \n",
       "1            2       True          4  \n",
       "2            2       True          7  \n",
       "3            2       True          5  \n",
       "4            1       True          1  \n",
       "5            1       True          3  \n",
       "6            1       True          2  \n",
       "7            2       True          6  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0cd7dcac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-25T12:12:32.590565Z",
     "iopub.status.busy": "2025-01-25T12:12:32.590277Z",
     "iopub.status.idle": "2025-01-25T12:17:21.644430Z",
     "shell.execute_reply": "2025-01-25T12:17:21.643513Z"
    },
    "papermill": {
     "duration": 289.106727,
     "end_time": "2025-01-25T12:17:21.670955",
     "exception": false,
     "start_time": "2025-01-25T12:12:32.564228",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        19.323559\n",
       "1        30.072632\n",
       "2        27.984558\n",
       "3        21.619816\n",
       "4        23.280861\n",
       "           ...    \n",
       "98545    21.191885\n",
       "98546    43.842415\n",
       "98547    40.399117\n",
       "98548    32.118626\n",
       "98549    34.800621\n",
       "Name: num_sold, Length: 98550, dtype: float32"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = predictor.predict(test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86dedf1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-25T12:17:21.724115Z",
     "iopub.status.busy": "2025-01-25T12:17:21.723812Z",
     "iopub.status.idle": "2025-01-25T12:17:21.728805Z",
     "shell.execute_reply": "2025-01-25T12:17:21.728110Z"
    },
    "papermill": {
     "duration": 0.033722,
     "end_time": "2025-01-25T12:17:21.730045",
     "exception": false,
     "start_time": "2025-01-25T12:17:21.696323",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inverse_boxcox(y_transformed, lmbda):\n",
    "    if lmbda==0:\n",
    "        return np.exp(y_transformed)\n",
    "    else:\n",
    "        return np.power(y_transformed*lmbda+1, 1/lmbda)\n",
    "# preds = inverse_quantile(preds)\n",
    "preds = inverse_boxcox(predictions, lmbda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48e195a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-25T12:17:21.782484Z",
     "iopub.status.busy": "2025-01-25T12:17:21.782199Z",
     "iopub.status.idle": "2025-01-25T12:17:21.891801Z",
     "shell.execute_reply": "2025-01-25T12:17:21.890788Z"
    },
    "papermill": {
     "duration": 0.137209,
     "end_time": "2025-01-25T12:17:21.893548",
     "exception": false,
     "start_time": "2025-01-25T12:17:21.756339",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_test = pd.read_csv('/kaggle/input/playground-series-s5e1/test.csv')\n",
    "idx = sample_test['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c92ad323",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-25T12:17:21.945654Z",
     "iopub.status.busy": "2025-01-25T12:17:21.945320Z",
     "iopub.status.idle": "2025-01-25T12:17:22.066971Z",
     "shell.execute_reply": "2025-01-25T12:17:22.065986Z"
    },
    "papermill": {
     "duration": 0.149266,
     "end_time": "2025-01-25T12:17:22.068540",
     "exception": false,
     "start_time": "2025-01-25T12:17:21.919274",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions_df = pd.DataFrame({\n",
    "    'id':idx,\n",
    "    'num_sold':preds\n",
    "})\n",
    "predictions_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883edc25",
   "metadata": {
    "papermill": {
     "duration": 0.024894,
     "end_time": "2025-01-25T12:17:22.118892",
     "exception": false,
     "start_time": "2025-01-25T12:17:22.093998",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 10652996,
     "sourceId": 85723,
     "sourceType": "competition"
    },
    {
     "datasetId": 6534356,
     "sourceId": 10561254,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6534615,
     "sourceId": 10561580,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6534758,
     "sourceId": 10561751,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3677.409704,
   "end_time": "2025-01-25T12:17:24.671621",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-25T11:16:07.261917",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
