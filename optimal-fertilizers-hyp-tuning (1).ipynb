{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91717,"databundleVersionId":12184666,"sourceType":"competition"},{"sourceId":11592231,"sourceType":"datasetVersion","datasetId":7269189}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-15T18:34:35.354286Z","iopub.execute_input":"2025-06-15T18:34:35.354528Z","iopub.status.idle":"2025-06-15T18:34:37.237348Z","shell.execute_reply.started":"2025-06-15T18:34:35.354500Z","shell.execute_reply":"2025-06-15T18:34:37.236531Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/playground-series-s5e6/sample_submission.csv\n/kaggle/input/playground-series-s5e6/train.csv\n/kaggle/input/playground-series-s5e6/test.csv\n/kaggle/input/fertilizer-prediction/Fertilizer Prediction.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"seed=1\nfolds=5\nk=3\ntrial=150","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T18:34:37.238175Z","iopub.execute_input":"2025-06-15T18:34:37.238558Z","iopub.status.idle":"2025-06-15T18:34:37.242542Z","shell.execute_reply.started":"2025-06-15T18:34:37.238532Z","shell.execute_reply":"2025-06-15T18:34:37.241923Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"train_org = pd.read_csv('/kaggle/input/fertilizer-prediction/Fertilizer Prediction.csv')\ntrain = pd.read_csv('/kaggle/input/playground-series-s5e6/train.csv')\ntest = pd.read_csv('/kaggle/input/playground-series-s5e6/test.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T18:34:37.244475Z","iopub.execute_input":"2025-06-15T18:34:37.244721Z","iopub.status.idle":"2025-06-15T18:34:38.587638Z","shell.execute_reply.started":"2025-06-15T18:34:37.244702Z","shell.execute_reply":"2025-06-15T18:34:38.587018Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"train_new = pd.concat([train.drop(columns='id'), train_org], axis=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T18:34:38.588361Z","iopub.execute_input":"2025-06-15T18:34:38.588627Z","iopub.status.idle":"2025-06-15T18:34:38.682077Z","shell.execute_reply.started":"2025-06-15T18:34:38.588598Z","shell.execute_reply":"2025-06-15T18:34:38.681490Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"X = train_new.drop(columns=['Fertilizer Name'])\ny = train_new['Fertilizer Name']\nclass_mapping = {\n    '14-35-14':0,\n    '10-26-26':1,\n    '17-17-17':2,\n    '28-28':3,\n    '20-20':4,\n    'DAP':5,\n    'Urea':6\n}\nrev_class_mapping = {\n    0:'14-35-14',\n    1:'10-26-26',\n    2:'17-17-17',\n    3:'28-28',\n    4:'20-20',\n    5:'DAP',\n    6:'Urea'\n}\ny = y.map(class_mapping)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T18:34:38.682909Z","iopub.execute_input":"2025-06-15T18:34:38.683193Z","iopub.status.idle":"2025-06-15T18:34:38.766560Z","shell.execute_reply.started":"2025-06-15T18:34:38.683168Z","shell.execute_reply":"2025-06-15T18:34:38.765964Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def trial_domain_features(df):\n    df = df.copy()\n    df['N_deficient'] = df['Nitrogen'] < 20\n    df['P_deficient'] = df['Phosphorous'] < 10  \n    df['K_deficient'] = df['Potassium'] < 15\n    return df.copy()\n\nX = trial_domain_features(X)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T18:34:38.767403Z","iopub.execute_input":"2025-06-15T18:34:38.767630Z","iopub.status.idle":"2025-06-15T18:34:38.872806Z","shell.execute_reply.started":"2025-06-15T18:34:38.767612Z","shell.execute_reply":"2025-06-15T18:34:38.872018Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"cols = X.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T18:34:38.873694Z","iopub.execute_input":"2025-06-15T18:34:38.874138Z","iopub.status.idle":"2025-06-15T18:34:38.877912Z","shell.execute_reply.started":"2025-06-15T18:34:38.874088Z","shell.execute_reply":"2025-06-15T18:34:38.877240Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"X[cols] = X[cols].astype('category')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T18:34:38.878763Z","iopub.execute_input":"2025-06-15T18:34:38.879469Z","iopub.status.idle":"2025-06-15T18:34:39.098745Z","shell.execute_reply.started":"2025-06-15T18:34:38.879448Z","shell.execute_reply":"2025-06-15T18:34:39.097903Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.model_selection import StratifiedKFold\nimport optuna\nfrom optuna.pruners import HyperbandPruner, MedianPruner\nfrom optuna.samplers import TPESampler\nfrom sklearn.metrics import make_scorer\n\ndef mapk_score(y_true, y_score, k=3):\n    sorted_predictions = np.argsort(y_score, axis=1)[:, -k:][:, ::-1]\n    map_at_k = 0\n    for i in range(k):\n        map_at_k += (sorted_predictions[:, i] == y_true).sum() / (i+1)\n    return map_at_k / len(y_score)\n\ndef xgb_objective(trial):\n    params = {\n        'tree_method': 'hist',  \n        'predictor': 'gpu_predictor',\n        'device': 'cuda',\n        'random_state': seed,\n        'objective': 'multi:softprob',\n        'num_class': len(np.unique(y)),\n        'verbosity': 0,\n        'eval_metric': 'mlogloss',\n        'enable_categorical': True,\n        \n        # VERY conservative for coarse search\n        'n_estimators': trial.suggest_int('n_estimators', 500, 5000),\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n        'max_depth': trial.suggest_int('max_depth', 3, 30),\n        'min_child_weight': trial.suggest_float('min_child_weight', 1, 25),\n        'gamma': trial.suggest_float('gamma', 0, 2),\n        \n        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n        \n        'reg_alpha': trial.suggest_float('reg_alpha', 0, 5),\n        'reg_lambda': trial.suggest_float('reg_lambda', 0, 5),\n        \n        'grow_policy': 'depthwise',\n        'max_bin': trial.suggest_categorical('max_bin', [64, 128, 256]),\n        'max_cat_threshold': trial.suggest_int('max_cat_threshold', 4, 32),\n        'max_cat_to_onehot': trial.suggest_int('max_cat_to_onehot', 2, 6),\n        \n        'early_stopping_rounds': 100,\n    }\n    \n    # If grow_policy is depthwise, max_leaves should be 0\n    if params['grow_policy'] == 'depthwise':\n        params['max_leaves'] = 0\n    \n    scores = []\n    skf = StratifiedKFold(random_state=seed, n_splits=folds, shuffle=True)\n    \n    for fold, (train_index, valid_index) in enumerate(skf.split(X, y), 1):\n        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n        \n        # Create model\n        model = xgb.XGBClassifier(**params)\n        \n        # Fit with early stopping\n        model.fit(\n            X_train, y_train,\n            eval_set=[(X_valid, y_valid)],\n            verbose=False\n        )\n        \n        # Get predictions and calculate MAP@3\n        probas = model.predict_proba(X_valid)\n        score = mapk_score(y_valid, probas, k=k)\n        scores.append(score)\n        \n        # Report intermediate score for pruning after 2 folds\n        if fold >= 2:\n            intermediate_score = np.mean(scores)\n            trial.report(intermediate_score, fold)\n            \n            # Check if trial should be pruned\n            if trial.should_prune():\n                raise optuna.exceptions.TrialPruned()\n    \n    return np.mean(scores)\n\n# sampler = TPESampler(\n#     n_startup_trials=30,\n#     n_ei_candidates=100,\n#     multivariate=True,\n#     group=True,\n#     warn_independent_sampling=False\n# )\n\n# pruner = MedianPruner(\n#     n_startup_trials=100,      # Let first 30 trials complete fully to establish median baseline\n#     n_warmup_steps=2,         # After trial 30, start pruning after 2 folds (not before)\n#     interval_steps=1          # Check for pruning after every fold (once warmup is done)\n# )\n# pruner = HyperbandPruner(\n#     min_resource=2,\n#     max_resource=folds,\n#     reduction_factor=3,\n#     bootstrap_count=5\n# )\n\nxgb_study = optuna.create_study(\n    direction='maximize',\n    sampler=TPESampler(n_startup_trials=10),\n    pruner=MedianPruner(n_startup_trials=10, n_warmup_steps=2),\n    study_name='xgb_optimization'\n)\n\nxgb_study.optimize(xgb_objective, n_trials = trial, show_progress_bar=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T18:34:39.100793Z","iopub.execute_input":"2025-06-15T18:34:39.101008Z"}},"outputs":[{"name":"stderr","text":"[I 2025-06-15 18:34:41,172] A new study created in memory with name: xgb_optimization\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/150 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c29fbdae90cc4f7c83fafa98b7534035"}},"metadata":{}},{"name":"stdout","text":"[I 2025-06-15 18:41:17,575] Trial 0 finished with value: 0.3016996078431372 and parameters: {'n_estimators': 3944, 'learning_rate': 0.22879932221072577, 'max_depth': 28, 'min_child_weight': 3.960708205594546, 'gamma': 1.4015859765469676, 'subsample': 0.7567931196448453, 'colsample_bytree': 0.868376974866955, 'reg_alpha': 0.32845445325463185, 'reg_lambda': 3.2568651335593497, 'max_bin': 256, 'max_cat_threshold': 31, 'max_cat_to_onehot': 3}. Best is trial 0 with value: 0.3016996078431372.\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"print(\"Best trial:\")\ntrial = study_xgb.best_trial\nprint(f\"  RMSLE: {trial.value:.5f}\")\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(f\"    {key}: {value}\")\n\n# Train final model\nbest_params = study_xgb.best_params.copy()\nbest_params.update({'early_stopping_rounds': None}) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}